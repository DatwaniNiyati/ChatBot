{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "colab_type": "code",
    "id": "kiMsEi4PhAUc",
    "outputId": "26cfee9d-9f3e-44ec-c78f-991439bd065d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from keras.layers import Input, Embedding, LSTM, TimeDistributed, Dense, Bidirectional\n",
    "from keras.models import Model, load_model\n",
    "\n",
    "INPUT_LENGTH = 20\n",
    "OUTPUT_LENGTH = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7tUP_8yWlkAL"
   },
   "outputs": [],
   "source": [
    "lines = open('movie_lines.txt',encoding = 'utf-8',errors = 'ignore').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QpnM--bkl0H0"
   },
   "outputs": [],
   "source": [
    "conv_lines = open('movie_conversations.txt',encoding = 'utf-8',errors = 'ignore').read().split('\\n')\n",
    "# conv_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KOQ48MCnl8Rc"
   },
   "outputs": [],
   "source": [
    "# Create a dictionary to map each line's id with its text\n",
    "id2line = {}\n",
    "for line in lines:\n",
    "    _line = line.split(' +++$+++ ')\n",
    "    if len(_line) == 5:\n",
    "        id2line[_line[0]] = _line[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bdWRXyfQmAB1"
   },
   "outputs": [],
   "source": [
    "# Create a list of all of the conversations' lines' ids.\n",
    "convs = []\n",
    "for line in conv_lines[:-1]:\n",
    "    _line = line.split(' +++$+++ ')[-1][1:-1].replace(\"'\",\"\").replace(\" \",\"\")\n",
    "    convs.append(_line.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "FD-hQLK0mCuv",
    "outputId": "93da5f90-1f12-4867-d677-c1fd9ad4e66c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2563 Next.  Could I see your documents, please?\n",
      "L2564 Yes sir.\n"
     ]
    }
   ],
   "source": [
    "#id and conversation sample\n",
    "for k in convs[400]:\n",
    "    print (k, id2line[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "blmJmtcsmFjQ",
    "outputId": "f4cb6ab2-a4d3-4554-9c00-6323e2a332b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221616\n",
      "221616\n"
     ]
    }
   ],
   "source": [
    "# Sort the sentences into questions (inputs) and answers (targets)\n",
    "questions = []\n",
    "answers = []\n",
    "for conv in convs:\n",
    "    for i in range(len(conv)-1):\n",
    "        questions.append(id2line[conv[i]])\n",
    "        answers.append(id2line[conv[i+1]])\n",
    "        \n",
    "# Compare lengths of questions and answers\n",
    "print(len(questions))\n",
    "print(len(answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AovmAdyRmJJ4"
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''Clean text by removing unnecessary characters and altering the format of words.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"that is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"n'\", \"ng\", text)\n",
    "    text = re.sub(r\"'bout\", \"about\", text)\n",
    "    text = re.sub(r\"'til\", \"until\", text)\n",
    "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|]\", \"\", text)\n",
    "#     text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
    "    text = \" \".join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5gnBhtUwmOkm"
   },
   "outputs": [],
   "source": [
    "# Clean the data\n",
    "clean_questions = []\n",
    "for question in questions:\n",
    "    clean_questions.append(clean_text(question))\n",
    "clean_answers = []    \n",
    "for answer in answers:\n",
    "    clean_answers.append(clean_text(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "oj6vLiu1mRx_",
    "outputId": "08e98240-0996-49c2-ca5d-bbb251b5991a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.0\n",
      "19.0\n",
      "24.0\n",
      "32.0\n"
     ]
    }
   ],
   "source": [
    "# Find the length of sentences (not using nltk due to processing speed)\n",
    "lengths = []\n",
    "# lengths.append([len(nltk.word_tokenize(sent)) for sent in clean_questions]) #nltk approach\n",
    "for question in clean_questions:\n",
    "    lengths.append(len(question.split()))\n",
    "for answer in clean_answers:\n",
    "    lengths.append(len(answer.split()))\n",
    "# Create a dataframe so that the values can be inspected\n",
    "lengths = pd.DataFrame(lengths, columns=['counts'])\n",
    "print(np.percentile(lengths, 80))\n",
    "print(np.percentile(lengths, 85))\n",
    "print(np.percentile(lengths, 90))\n",
    "print(np.percentile(lengths, 95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "SCwoFE3WmWK5",
    "outputId": "18534ba5-f466-4ca5-8947-4b051776bc21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138528\n",
      "138528\n"
     ]
    }
   ],
   "source": [
    "# Remove questions and answers that are shorter than 1 word and longer than 20 words.\n",
    "min_line_length = 2\n",
    "max_line_length = 20\n",
    "\n",
    "# Filter out the questions that are too short/long\n",
    "short_questions_temp = []\n",
    "short_answers_temp = []\n",
    "\n",
    "for i, question in enumerate(clean_questions):\n",
    "    if len(question.split()) >= min_line_length and len(question.split()) <= max_line_length:\n",
    "        short_questions_temp.append(question)\n",
    "        short_answers_temp.append(clean_answers[i])\n",
    "\n",
    "# Filter out the answers that are too short/long\n",
    "short_questions = []\n",
    "short_answers = []\n",
    "\n",
    "for i, answer in enumerate(short_answers_temp):\n",
    "    if len(answer.split()) >= min_line_length and len(answer.split()) <= max_line_length:\n",
    "        short_answers.append(answer)\n",
    "        short_questions.append(short_questions_temp[i])\n",
    "        \n",
    "print(len(short_questions))\n",
    "print(len(short_answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "ZNXU2XovmaWp",
    "outputId": "c502ec4e-0530-4b1c-e851-01457e3eddbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "then how did you get here?\n",
      "i always eat here.\n",
      "\n",
      "i always eat here.\n",
      "this is a place for workmen.\n",
      "\n",
      "just an old man. his memory is getting weak.\n",
      "what are you after?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = np.random.randint(1,len(short_questions))\n",
    "\n",
    "for i in range(r, r+3):\n",
    "    print(short_questions[i])\n",
    "    print(short_answers[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "x4bJmZFQmttG",
    "outputId": "6dd9d2a5-d273-411e-a4cb-fa52d259f49d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Niyati\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Niyati\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt') # first-time use only\n",
    "nltk.download('wordnet') # first-time use only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v2bDfkoRmf_C"
   },
   "outputs": [],
   "source": [
    "#choosing number of samples\n",
    "num_samples = 30000  # Number of samples to train on.\n",
    "short_questions = short_questions[:num_samples]\n",
    "short_answers = short_answers[:num_samples]\n",
    "#tokenizing the qns and answers\n",
    "short_questions_tok = [nltk.word_tokenize(sent) for sent in short_questions]\n",
    "short_answers_tok = [nltk.word_tokenize(sent) for sent in short_answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "tNtVue42mjUJ",
    "outputId": "2c09152e-4330-4484-eb29-f94ad372005c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training size 24000\n",
      "validation size 6000\n"
     ]
    }
   ],
   "source": [
    "#train-validation split\n",
    "data_size = len(short_questions_tok)\n",
    "\n",
    "# We will use the first 0-80th %-tile (80%) of data for the training\n",
    "training_input  = short_questions_tok[:round(data_size*(80/100))]\n",
    "training_input  = [tr_input[::-1] for tr_input in training_input] #reverseing input seq for better performance\n",
    "training_output = short_answers_tok[:round(data_size*(80/100))]\n",
    "\n",
    "# We will use the remaining for validation\n",
    "validation_input = short_questions_tok[round(data_size*(80/100)):]\n",
    "validation_input  = [val_input[::-1] for val_input in validation_input] #reverseing input seq for better performance\n",
    "validation_output = short_answers_tok[round(data_size*(80/100)):]\n",
    "\n",
    "print('training size', len(training_input))\n",
    "print('validation size', len(validation_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xdcwy3xYm6Zq"
   },
   "outputs": [],
   "source": [
    "# Create a dictionary for the frequency of the vocabulary\n",
    "# Create \n",
    "vocab = {}\n",
    "for question in short_questions_tok:\n",
    "    for word in question:\n",
    "        if word not in vocab:\n",
    "            vocab[word] = 1\n",
    "        else:\n",
    "            vocab[word] += 1\n",
    "\n",
    "for answer in short_answers_tok:\n",
    "    for word in answer:\n",
    "        if word not in vocab:\n",
    "            vocab[word] = 1\n",
    "        else:\n",
    "            vocab[word] += 1         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QzYrXgMYm_GZ"
   },
   "outputs": [],
   "source": [
    "# Remove rare words from the vocabulary.\n",
    "# We will aim to replace fewer than 5% of words with <UNK>\n",
    "# You will see this ratio soon.\n",
    "threshold = 15\n",
    "count = 0\n",
    "for k,v in vocab.items():\n",
    "    if v >= threshold:\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "-6dHWJ_5nCA4",
    "outputId": "69001a59-a66c-4969-cd1f-b7da3c34789d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of total vocab: 16382\n",
      "Size of vocab we will use: 1946\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of total vocab:\", len(vocab))\n",
    "print(\"Size of vocab we will use:\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ubCkeCpznE7x",
    "outputId": "7833135a-eef4-46b9-bdbe-0ec3b4366756"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of vocab used: 1948\n"
     ]
    }
   ],
   "source": [
    "#we will create dictionaries to provide a unique integer for each word.\n",
    "WORD_CODE_START = 1\n",
    "WORD_CODE_PADDING = 0\n",
    "\n",
    "\n",
    "word_num  = 2 #number 1 is left for WORD_CODE_START for model decoder later\n",
    "encoding = {}\n",
    "decoding = {1: 'START'}\n",
    "for word, count in vocab.items():\n",
    "    if count >= threshold: #get vocabularies that appear above threshold count\n",
    "        encoding[word] = word_num \n",
    "        decoding[word_num ] = word\n",
    "        word_num += 1\n",
    "\n",
    "print(\"No. of vocab used:\", word_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2BlbAOVYnIuR"
   },
   "outputs": [],
   "source": [
    "#include unknown token for words not in dictionary\n",
    "decoding[len(encoding)+2] = '<UNK>'\n",
    "encoding['<UNK>'] = len(encoding)+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "zsm9GD1ynMNa",
    "outputId": "8a63adf5-ee67-4092-e894-861f80c54456"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1949"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_size = word_num+1\n",
    "dict_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v-jfL9Q2nOFe"
   },
   "outputs": [],
   "source": [
    "def transform(encoding, data, vector_size=20):\n",
    "    \"\"\"\n",
    "    :param encoding: encoding dict built by build_word_encoding()\n",
    "    :param data: list of strings\n",
    "    :param vector_size: size of each encoded vector\n",
    "    \"\"\"\n",
    "    transformed_data = np.zeros(shape=(len(data), vector_size))\n",
    "    for i in range(len(data)):\n",
    "        for j in range(min(len(data[i]), vector_size)):\n",
    "            try:\n",
    "                transformed_data[i][j] = encoding[data[i][j]]\n",
    "            except:\n",
    "                transformed_data[i][j] = encoding['<UNK>']\n",
    "    return transformed_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "EbLlVWH6nSW4",
    "outputId": "7fc978b1-9d35-48b4-8a3d-2a7fca6527d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded_training_input (24000, 20)\n",
      "encoded_training_output (24000, 20)\n"
     ]
    }
   ],
   "source": [
    "#encoding training set\n",
    "encoded_training_input = transform(\n",
    "    encoding, training_input, vector_size=INPUT_LENGTH)\n",
    "encoded_training_output = transform(\n",
    "    encoding, training_output, vector_size=OUTPUT_LENGTH)\n",
    "\n",
    "print('encoded_training_input', encoded_training_input.shape)\n",
    "print('encoded_training_output', encoded_training_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "NeZGtmBLnW3A",
    "outputId": "cd67dc8d-ae93-46c8-a2df-4d518d1c8d1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded_validation_input (6000, 20)\n",
      "encoded_validation_output (6000, 20)\n"
     ]
    }
   ],
   "source": [
    "#encoding validation set\n",
    "encoded_validation_input = transform(\n",
    "    encoding, validation_input, vector_size=INPUT_LENGTH)\n",
    "encoded_validation_output = transform(\n",
    "    encoding, validation_output, vector_size=OUTPUT_LENGTH)\n",
    "\n",
    "print('encoded_validation_input', encoded_validation_input.shape)\n",
    "print('encoded_validation_output', encoded_validation_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "idpEx_jtnZPd",
    "outputId": "52eebac7-469a-4591-b87c-40ca82b23e97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.keras.backend.clear_session()\n",
    "tf.autograph.set_verbosity(0)\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "aUfRG42KndwD",
    "outputId": "9631aa99-639e-405f-a68e-e1359e275e20"
   },
   "outputs": [],
   "source": [
    "INPUT_LENGTH = 20\n",
    "OUTPUT_LENGTH = 20\n",
    "\n",
    "encoder_input = Input(shape=(INPUT_LENGTH,))\n",
    "decoder_input = Input(shape=(OUTPUT_LENGTH,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "U5lR6YryngQ3",
    "outputId": "c13227c0-8ba8-48c2-b66a-54cdb3ba63b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Niyati\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From C:\\Users\\Niyati\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\keras\\backend.py:3872: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "encoder Tensor(\"lstm_1/transpose_2:0\", shape=(?, 20, 512), dtype=float32)\n",
      "encoder_last Tensor(\"strided_slice:0\", shape=(?, 512), dtype=float32)\n",
      "decoder Tensor(\"lstm_2/transpose_2:0\", shape=(?, 20, 512), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import SimpleRNN\n",
    "\n",
    "encoder = Embedding(dict_size, 128, input_length=INPUT_LENGTH, mask_zero=True)(encoder_input)\n",
    "encoder = LSTM(512, return_sequences=True, unroll=True)(encoder)\n",
    "encoder_last = encoder[:,-1,:]\n",
    "\n",
    "print('encoder', encoder)\n",
    "print('encoder_last', encoder_last)\n",
    "\n",
    "decoder = Embedding(dict_size, 128, input_length=OUTPUT_LENGTH, mask_zero=True)(decoder_input)\n",
    "decoder = LSTM(512, return_sequences=True, unroll=True)(decoder, initial_state=[encoder_last, encoder_last])\n",
    "\n",
    "print('decoder', decoder)\n",
    "\n",
    "# For the plain Sequence-to-Sequence, we produced the output from directly from decoder\n",
    "# output = TimeDistributed(Dense(output_dict_size, activation=\"softmax\"))(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "aAZpVIjKnnrk",
    "outputId": "9eaf0830-23cc-4a27-d1f3-123ce9d603c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention Tensor(\"attention/truediv:0\", shape=(?, 20, 20), dtype=float32)\n",
      "context Tensor(\"dot_2/MatMul:0\", shape=(?, 20, 512), dtype=float32)\n",
      "decoder_combined_context Tensor(\"concatenate_1/concat:0\", shape=(?, 20, 1024), dtype=float32)\n",
      "output Tensor(\"time_distributed_2/Reshape_1:0\", shape=(?, 20, 1949), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Activation, dot, concatenate\n",
    "\n",
    "attention = dot([decoder, encoder], axes=[2, 2])\n",
    "attention = Activation('softmax', name='attention')(attention)\n",
    "print('attention', attention)\n",
    "\n",
    "context = dot([attention, encoder], axes=[2,1])\n",
    "print('context', context)\n",
    "\n",
    "decoder_combined_context = concatenate([context, decoder])\n",
    "print('decoder_combined_context', decoder_combined_context)\n",
    "\n",
    "# Has another weight + tanh layer as described in equation (5) of the paper\n",
    "output = TimeDistributed(Dense(512, activation=\"tanh\"))(decoder_combined_context)\n",
    "output = TimeDistributed(Dense(dict_size, activation=\"softmax\"))(output)\n",
    "print('output', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 693
    },
    "colab_type": "code",
    "id": "0LMAJBf_nv6m",
    "outputId": "3a09d615-be54-4949-fac6-29d935debcc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 20, 128)      249472      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 20, 128)      249472      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 20, 512)      1312768     embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 20, 512)      1312768     embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 20, 20)       0           lstm_2[0][0]                     \n",
      "                                                                 lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "attention (Activation)          (None, 20, 20)       0           dot_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dot_2 (Dot)                     (None, 20, 512)      0           attention[0][0]                  \n",
      "                                                                 lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 20, 1024)     0           dot_2[0][0]                      \n",
      "                                                                 lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 20, 512)      524800      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 20, 1949)     999837      time_distributed_1[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 4,649,117\n",
      "Trainable params: 4,649,117\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[encoder_input, decoder_input], outputs=[output])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FLBxBZlxmxFs",
    "outputId": "faf74c98-b689-4d81-b320-08631d92d1c7"
   },
   "outputs": [],
   "source": [
    "model.layers\n",
    "len(model.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 624
    },
    "colab_type": "code",
    "id": "oGNTI5H-nEjx",
    "outputId": "12b7ff7c-5417-4fec-f806-acdc632db9db"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sYmyoMOxn1vU"
   },
   "outputs": [],
   "source": [
    "training_encoder_input = encoded_training_input\n",
    "training_decoder_input = np.zeros_like(encoded_training_output)\n",
    "training_decoder_input[:, 1:] = encoded_training_output[:,:-1]\n",
    "training_decoder_input[:, 0] = WORD_CODE_START\n",
    "training_decoder_output = np.eye(dict_size)[encoded_training_output.astype('int')]\n",
    "\n",
    "validation_encoder_input = encoded_validation_input\n",
    "validation_decoder_input = np.zeros_like(encoded_validation_output)\n",
    "validation_decoder_input[:, 1:] = encoded_validation_output[:,:-1]\n",
    "validation_decoder_input[:, 0] = WORD_CODE_START\n",
    "validation_decoder_output = np.eye(dict_size)[encoded_validation_output.astype('int')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kB5RtGm-CdTj"
   },
   "outputs": [],
   "source": [
    "# model_save_name = 'classifier.pt'\n",
    "# path = F\"/content/gdrive/My Drive/{model_save_name}\" \n",
    "# torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "aJC2SVFsn6Ad",
    "outputId": "2f210b59-51f2-48ad-833d-7ac50fc1b6f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\niyati\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 24000 samples, validate on 6000 samples\n",
      "Epoch 1/500\n",
      "24000/24000 [==============================] - 72s 3ms/step - loss: 0.0020 - val_loss: 0.0017\n",
      "Epoch 2/500\n",
      "24000/24000 [==============================] - 19s 783us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 3/500\n",
      "24000/24000 [==============================] - 19s 786us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 4/500\n",
      "24000/24000 [==============================] - 19s 787us/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 5/500\n",
      "24000/24000 [==============================] - 19s 786us/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 6/500\n",
      "24000/24000 [==============================] - 19s 790us/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 7/500\n",
      "24000/24000 [==============================] - 19s 808us/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 8/500\n",
      "24000/24000 [==============================] - 20s 832us/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 9/500\n",
      "24000/24000 [==============================] - 20s 839us/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 10/500\n",
      "24000/24000 [==============================] - 20s 813us/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 11/500\n",
      "24000/24000 [==============================] - 20s 839us/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 12/500\n",
      "24000/24000 [==============================] - 20s 848us/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 13/500\n",
      "24000/24000 [==============================] - 19s 811us/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 14/500\n",
      "24000/24000 [==============================] - 20s 820us/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 15/500\n",
      "24000/24000 [==============================] - 20s 835us/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 16/500\n",
      "24000/24000 [==============================] - 20s 818us/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 17/500\n",
      "24000/24000 [==============================] - 20s 842us/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 18/500\n",
      "24000/24000 [==============================] - 21s 867us/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 19/500\n",
      "24000/24000 [==============================] - 20s 831us/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 20/500\n",
      "24000/24000 [==============================] - 20s 820us/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 21/500\n",
      "24000/24000 [==============================] - 20s 829us/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 22/500\n",
      "24000/24000 [==============================] - 20s 823us/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 23/500\n",
      "24000/24000 [==============================] - 20s 816us/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 24/500\n",
      "24000/24000 [==============================] - 19s 808us/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 25/500\n",
      "24000/24000 [==============================] - 20s 823us/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 26/500\n",
      "24000/24000 [==============================] - 20s 820us/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 27/500\n",
      "24000/24000 [==============================] - 20s 823us/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 28/500\n",
      "24000/24000 [==============================] - 19s 806us/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 29/500\n",
      "24000/24000 [==============================] - 20s 818us/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 30/500\n",
      "24000/24000 [==============================] - 20s 814us/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 31/500\n",
      "24000/24000 [==============================] - 20s 841us/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 32/500\n",
      "24000/24000 [==============================] - 20s 821us/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 33/500\n",
      "24000/24000 [==============================] - 19s 812us/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 34/500\n",
      "24000/24000 [==============================] - 19s 802us/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 35/500\n",
      "24000/24000 [==============================] - 19s 805us/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 36/500\n",
      "24000/24000 [==============================] - 20s 817us/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 37/500\n",
      "24000/24000 [==============================] - 19s 809us/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 38/500\n",
      "24000/24000 [==============================] - 20s 814us/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 39/500\n",
      "24000/24000 [==============================] - 20s 813us/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 40/500\n",
      "24000/24000 [==============================] - 19s 812us/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 41/500\n",
      "24000/24000 [==============================] - 20s 813us/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 42/500\n",
      "24000/24000 [==============================] - 20s 822us/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 43/500\n",
      "24000/24000 [==============================] - 20s 813us/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 44/500\n",
      "24000/24000 [==============================] - 20s 822us/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 45/500\n",
      "24000/24000 [==============================] - 20s 819us/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 46/500\n",
      "24000/24000 [==============================] - 20s 824us/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 47/500\n",
      "24000/24000 [==============================] - 20s 823us/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 48/500\n",
      "24000/24000 [==============================] - 20s 827us/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 49/500\n",
      "24000/24000 [==============================] - 20s 820us/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 50/500\n",
      "24000/24000 [==============================] - 20s 816us/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 51/500\n",
      "24000/24000 [==============================] - 19s 811us/step - loss: 0.0010 - val_loss: 0.0013\n",
      "Epoch 52/500\n",
      "24000/24000 [==============================] - 19s 799us/step - loss: 0.0010 - val_loss: 0.0013\n",
      "Epoch 53/500\n",
      "24000/24000 [==============================] - 19s 786us/step - loss: 0.0010 - val_loss: 0.0013\n",
      "Epoch 54/500\n",
      "24000/24000 [==============================] - 18s 766us/step - loss: 0.0010 - val_loss: 0.0013\n",
      "Epoch 55/500\n",
      "24000/24000 [==============================] - 19s 782us/step - loss: 0.0010 - val_loss: 0.0013\n",
      "Epoch 56/500\n",
      "24000/24000 [==============================] - 19s 776us/step - loss: 9.9892e-04 - val_loss: 0.0013\n",
      "Epoch 57/500\n",
      "24000/24000 [==============================] - 19s 787us/step - loss: 9.8996e-04 - val_loss: 0.0013\n",
      "Epoch 58/500\n",
      "24000/24000 [==============================] - 19s 791us/step - loss: 9.8044e-04 - val_loss: 0.0013\n",
      "Epoch 59/500\n",
      "24000/24000 [==============================] - 19s 791us/step - loss: 9.7117e-04 - val_loss: 0.0013\n",
      "Epoch 60/500\n",
      "24000/24000 [==============================] - 20s 822us/step - loss: 9.6111e-04 - val_loss: 0.0013\n",
      "Epoch 61/500\n",
      "24000/24000 [==============================] - 20s 814us/step - loss: 9.5168e-04 - val_loss: 0.0013\n",
      "Epoch 62/500\n",
      "24000/24000 [==============================] - 20s 824us/step - loss: 9.4171e-04 - val_loss: 0.0014\n",
      "Epoch 63/500\n",
      "24000/24000 [==============================] - 20s 815us/step - loss: 9.3160e-04 - val_loss: 0.0014\n",
      "Epoch 64/500\n",
      "24000/24000 [==============================] - 19s 803us/step - loss: 9.2154e-04 - val_loss: 0.0014\n",
      "Epoch 65/500\n",
      "24000/24000 [==============================] - 20s 816us/step - loss: 9.1183e-04 - val_loss: 0.0014\n",
      "Epoch 66/500\n",
      "24000/24000 [==============================] - 20s 818us/step - loss: 9.0268e-04 - val_loss: 0.0014\n",
      "Epoch 67/500\n",
      "24000/24000 [==============================] - 19s 810us/step - loss: 8.9288e-04 - val_loss: 0.0014\n",
      "Epoch 68/500\n",
      "24000/24000 [==============================] - 19s 811us/step - loss: 8.8237e-04 - val_loss: 0.0014\n",
      "Epoch 69/500\n",
      "24000/24000 [==============================] - 20s 844us/step - loss: 8.7267e-04 - val_loss: 0.0014\n",
      "Epoch 70/500\n",
      "24000/24000 [==============================] - 21s 859us/step - loss: 8.6251e-04 - val_loss: 0.0014\n",
      "Epoch 71/500\n",
      "24000/24000 [==============================] - 21s 860us/step - loss: 8.5272e-04 - val_loss: 0.0014\n",
      "Epoch 72/500\n",
      "24000/24000 [==============================] - 21s 860us/step - loss: 8.4362e-04 - val_loss: 0.0014TA: 0s - loss: 8.4266e\n",
      "Epoch 73/500\n",
      "24000/24000 [==============================] - 21s 871us/step - loss: 8.3372e-04 - val_loss: 0.0014\n",
      "Epoch 74/500\n",
      "24000/24000 [==============================] - 20s 849us/step - loss: 8.2363e-04 - val_loss: 0.0015\n",
      "Epoch 75/500\n",
      "24000/24000 [==============================] - 20s 852us/step - loss: 8.1389e-04 - val_loss: 0.0015\n",
      "Epoch 76/500\n",
      "24000/24000 [==============================] - 21s 858us/step - loss: 8.0526e-04 - val_loss: 0.0015TA\n",
      "Epoch 77/500\n",
      "24000/24000 [==============================] - 21s 856us/step - loss: 7.9576e-04 - val_loss: 0.0015\n",
      "Epoch 78/500\n",
      "24000/24000 [==============================] - 20s 847us/step - loss: 7.8554e-04 - val_loss: 0.0015\n",
      "Epoch 79/500\n",
      "24000/24000 [==============================] - 20s 845us/step - loss: 7.7662e-04 - val_loss: 0.0015\n",
      "Epoch 80/500\n",
      "24000/24000 [==============================] - 20s 850us/step - loss: 7.6709e-04 - val_loss: 0.0015\n",
      "Epoch 81/500\n",
      "24000/24000 [==============================] - 21s 865us/step - loss: 7.5881e-04 - val_loss: 0.0015\n",
      "Epoch 82/500\n",
      "24000/24000 [==============================] - 21s 885us/step - loss: 7.4800e-04 - val_loss: 0.0015\n",
      "Epoch 83/500\n",
      "24000/24000 [==============================] - 21s 885us/step - loss: 7.3881e-04 - val_loss: 0.0015\n",
      "Epoch 84/500\n",
      "24000/24000 [==============================] - 21s 874us/step - loss: 7.2953e-04 - val_loss: 0.00160\n",
      "Epoch 85/500\n",
      "24000/24000 [==============================] - 21s 870us/step - loss: 7.2099e-04 - val_loss: 0.0016\n",
      "Epoch 86/500\n",
      "24000/24000 [==============================] - 21s 873us/step - loss: 7.1277e-04 - val_loss: 0.0016\n",
      "Epoch 87/500\n",
      "24000/24000 [==============================] - 21s 884us/step - loss: 7.0388e-04 - val_loss: 0.0016\n",
      "Epoch 88/500\n",
      "24000/24000 [==============================] - 21s 883us/step - loss: 6.9465e-04 - val_loss: 0.0016\n",
      "Epoch 89/500\n",
      "24000/24000 [==============================] - 21s 874us/step - loss: 6.8456e-04 - val_loss: 0.0016\n",
      "Epoch 90/500\n",
      "24000/24000 [==============================] - 21s 885us/step - loss: 6.7692e-04 - val_loss: 0.0016\n",
      "Epoch 91/500\n",
      "24000/24000 [==============================] - 23s 977us/step - loss: 6.6944e-04 - val_loss: 0.0016\n",
      "Epoch 92/500\n",
      "24000/24000 [==============================] - 23s 944us/step - loss: 6.6101e-04 - val_loss: 0.0016\n",
      "Epoch 93/500\n",
      "24000/24000 [==============================] - 21s 873us/step - loss: 6.5260e-04 - val_loss: 0.0017\n",
      "Epoch 94/500\n",
      "24000/24000 [==============================] - 21s 865us/step - loss: 6.4431e-04 - val_loss: 0.0017\n",
      "Epoch 95/500\n",
      "24000/24000 [==============================] - 21s 876us/step - loss: 6.3575e-04 - val_loss: 0.0017\n",
      "Epoch 96/500\n",
      "24000/24000 [==============================] - 21s 864us/step - loss: 6.2786e-04 - val_loss: 0.0017\n",
      "Epoch 97/500\n",
      "24000/24000 [==============================] - 22s 896us/step - loss: 6.1872e-04 - val_loss: 0.0017\n",
      "Epoch 98/500\n",
      "24000/24000 [==============================] - 22s 909us/step - loss: 6.1144e-04 - val_loss: 0.0017\n",
      "Epoch 99/500\n",
      "24000/24000 [==============================] - 21s 869us/step - loss: 6.0356e-04 - val_loss: 0.0017\n",
      "Epoch 100/500\n",
      "24000/24000 [==============================] - 21s 863us/step - loss: 5.9570e-04 - val_loss: 0.0017\n",
      "Epoch 101/500\n",
      "24000/24000 [==============================] - 21s 865us/step - loss: 5.8789e-04 - val_loss: 0.0017\n",
      "Epoch 102/500\n",
      "24000/24000 [==============================] - 21s 868us/step - loss: 5.8029e-04 - val_loss: 0.0017\n",
      "Epoch 103/500\n",
      "24000/24000 [==============================] - 22s 903us/step - loss: 5.7250e-04 - val_loss: 0.0018\n",
      "Epoch 104/500\n",
      "24000/24000 [==============================] - 21s 894us/step - loss: 5.6540e-04 - val_loss: 0.0018\n",
      "Epoch 105/500\n",
      "24000/24000 [==============================] - 21s 873us/step - loss: 5.5992e-04 - val_loss: 0.0018\n",
      "Epoch 106/500\n",
      "24000/24000 [==============================] - 21s 865us/step - loss: 5.5211e-04 - val_loss: 0.0018\n",
      "Epoch 107/500\n",
      "24000/24000 [==============================] - 20s 853us/step - loss: 5.4353e-04 - val_loss: 0.0018\n",
      "Epoch 108/500\n",
      "24000/24000 [==============================] - 20s 843us/step - loss: 5.3626e-04 - val_loss: 0.0018\n",
      "Epoch 109/500\n",
      "24000/24000 [==============================] - 20s 839us/step - loss: 5.3065e-04 - val_loss: 0.0018\n",
      "Epoch 110/500\n",
      "24000/24000 [==============================] - 20s 840us/step - loss: 5.2246e-04 - val_loss: 0.0018\n",
      "Epoch 111/500\n",
      "24000/24000 [==============================] - 20s 844us/step - loss: 5.1656e-04 - val_loss: 0.0018\n",
      "Epoch 112/500\n",
      "24000/24000 [==============================] - 20s 846us/step - loss: 5.0857e-04 - val_loss: 0.0019\n",
      "Epoch 113/500\n",
      "24000/24000 [==============================] - 20s 840us/step - loss: 5.0569e-04 - val_loss: 0.0019\n",
      "Epoch 114/500\n",
      "24000/24000 [==============================] - 20s 847us/step - loss: 5.0071e-04 - val_loss: 0.0019\n",
      "Epoch 115/500\n",
      "24000/24000 [==============================] - 21s 859us/step - loss: 4.9315e-04 - val_loss: 0.0019\n",
      "Epoch 116/500\n",
      "24000/24000 [==============================] - 21s 871us/step - loss: 4.8416e-04 - val_loss: 0.0019\n",
      "Epoch 117/500\n",
      "24000/24000 [==============================] - 21s 873us/step - loss: 4.7737e-04 - val_loss: 0.0019\n",
      "Epoch 118/500\n",
      "24000/24000 [==============================] - 21s 869us/step - loss: 4.7115e-04 - val_loss: 0.0019\n",
      "Epoch 119/500\n",
      "24000/24000 [==============================] - 22s 908us/step - loss: 4.6751e-04 - val_loss: 0.0019\n",
      "Epoch 120/500\n",
      "24000/24000 [==============================] - 21s 876us/step - loss: 4.6364e-04 - val_loss: 0.0019\n",
      "Epoch 121/500\n",
      "24000/24000 [==============================] - 21s 877us/step - loss: 4.5466e-04 - val_loss: 0.0019\n",
      "Epoch 122/500\n",
      "24000/24000 [==============================] - 21s 860us/step - loss: 4.4753e-04 - val_loss: 0.0020\n",
      "Epoch 123/500\n",
      "24000/24000 [==============================] - 21s 888us/step - loss: 4.4276e-04 - val_loss: 0.0020\n",
      "Epoch 124/500\n",
      "24000/24000 [==============================] - 21s 882us/step - loss: 4.3837e-04 - val_loss: 0.0020\n",
      "Epoch 125/500\n",
      "24000/24000 [==============================] - 21s 859us/step - loss: 4.3221e-04 - val_loss: 0.0020\n",
      "Epoch 126/500\n",
      "24000/24000 [==============================] - 21s 859us/step - loss: 4.2887e-04 - val_loss: 0.0020\n",
      "Epoch 127/500\n",
      "24000/24000 [==============================] - 21s 856us/step - loss: 4.2093e-04 - val_loss: 0.0020\n",
      "Epoch 128/500\n",
      "24000/24000 [==============================] - 21s 856us/step - loss: 4.1360e-04 - val_loss: 0.0020\n",
      "Epoch 129/500\n",
      "24000/24000 [==============================] - 22s 925us/step - loss: 4.0496e-04 - val_loss: 0.0020\n",
      "Epoch 130/500\n",
      "24000/24000 [==============================] - 22s 898us/step - loss: 3.9969e-04 - val_loss: 0.0020\n",
      "Epoch 131/500\n",
      "24000/24000 [==============================] - 21s 888us/step - loss: 3.9359e-04 - val_loss: 0.0020\n",
      "Epoch 132/500\n",
      "24000/24000 [==============================] - 21s 876us/step - loss: 3.8922e-04 - val_loss: 0.0021\n",
      "Epoch 133/500\n",
      "24000/24000 [==============================] - 21s 877us/step - loss: 3.8996e-04 - val_loss: 0.0021\n",
      "Epoch 134/500\n",
      "24000/24000 [==============================] - 21s 879us/step - loss: 3.9625e-04 - val_loss: 0.0021\n",
      "Epoch 135/500\n",
      "24000/24000 [==============================] - 21s 885us/step - loss: 4.0205e-04 - val_loss: 0.0021\n",
      "Epoch 136/500\n",
      "24000/24000 [==============================] - 22s 896us/step - loss: 4.0212e-04 - val_loss: 0.0021\n",
      "Epoch 137/500\n",
      "24000/24000 [==============================] - 21s 876us/step - loss: 3.9337e-04 - val_loss: 0.0021\n",
      "Epoch 138/500\n",
      "24000/24000 [==============================] - 21s 885us/step - loss: 3.7983e-04 - val_loss: 0.0021\n",
      "Epoch 139/500\n",
      "24000/24000 [==============================] - 22s 898us/step - loss: 3.6276e-04 - val_loss: 0.0021\n",
      "Epoch 140/500\n",
      "24000/24000 [==============================] - 21s 884us/step - loss: 3.4867e-04 - val_loss: 0.0021\n",
      "Epoch 141/500\n",
      "24000/24000 [==============================] - 21s 879us/step - loss: 3.3847e-04 - val_loss: 0.0021\n",
      "Epoch 142/500\n",
      "24000/24000 [==============================] - 21s 877us/step - loss: 3.3088e-04 - val_loss: 0.0021\n",
      "Epoch 143/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000/24000 [==============================] - 21s 882us/step - loss: 3.2555e-04 - val_loss: 0.0022\n",
      "Epoch 144/500\n",
      "24000/24000 [==============================] - 24s 990us/step - loss: 3.2006e-04 - val_loss: 0.0022\n",
      "Epoch 145/500\n",
      "24000/24000 [==============================] - 26s 1ms/step - loss: 3.1570e-04 - val_loss: 0.0022\n",
      "Epoch 146/500\n",
      "24000/24000 [==============================] - 25s 1ms/step - loss: 3.1650e-04 - val_loss: 0.0022\n",
      "Epoch 147/500\n",
      "24000/24000 [==============================] - 25s 1ms/step - loss: 3.2451e-04 - val_loss: 0.0022\n",
      "Epoch 148/500\n",
      "24000/24000 [==============================] - 34s 1ms/step - loss: 3.8405e-04 - val_loss: 0.0022\n",
      "Epoch 149/500\n",
      "24000/24000 [==============================] - 23s 947us/step - loss: 4.2368e-04 - val_loss: 0.0022\n",
      "Epoch 150/500\n",
      "24000/24000 [==============================] - 23s 947us/step - loss: 3.8760e-04 - val_loss: 0.0022\n",
      "Epoch 151/500\n",
      "24000/24000 [==============================] - 31s 1ms/step - loss: 3.4171e-04 - val_loss: 0.0022\n",
      "Epoch 152/500\n",
      "24000/24000 [==============================] - 25s 1ms/step - loss: 3.1253e-04 - val_loss: 0.0022\n",
      "Epoch 153/500\n",
      "24000/24000 [==============================] - 30s 1ms/step - loss: 2.9412e-04 - val_loss: 0.0022\n",
      "Epoch 154/500\n",
      "24000/24000 [==============================] - 31s 1ms/step - loss: 2.8244e-04 - val_loss: 0.0022\n",
      "Epoch 155/500\n",
      "24000/24000 [==============================] - 30s 1ms/step - loss: 2.7389e-04 - val_loss: 0.0022\n",
      "Epoch 156/500\n",
      "24000/24000 [==============================] - 30s 1ms/step - loss: 2.6808e-04 - val_loss: 0.0022\n",
      "Epoch 157/500\n",
      "24000/24000 [==============================] - 29s 1ms/step - loss: 2.6269e-04 - val_loss: 0.0023\n",
      "Epoch 158/500\n",
      "24000/24000 [==============================] - 30s 1ms/step - loss: 2.5804e-04 - val_loss: 0.0023\n",
      "Epoch 159/500\n",
      "24000/24000 [==============================] - 30s 1ms/step - loss: 2.5402e-04 - val_loss: 0.0023\n",
      "Epoch 160/500\n",
      "24000/24000 [==============================] - 29s 1ms/step - loss: 2.4979e-04 - val_loss: 0.0023\n",
      "Epoch 161/500\n",
      "24000/24000 [==============================] - 29s 1ms/step - loss: 2.4787e-04 - val_loss: 0.0023\n",
      "Epoch 162/500\n",
      "24000/24000 [==============================] - 30s 1ms/step - loss: 2.4901e-04 - val_loss: 0.0023\n",
      "Epoch 163/500\n",
      "24000/24000 [==============================] - 33s 1ms/step - loss: 2.8160e-04 - val_loss: 0.0023\n",
      "Epoch 164/500\n",
      "24000/24000 [==============================] - 29s 1ms/step - loss: 3.7530e-04 - val_loss: 0.0023\n",
      "Epoch 165/500\n",
      "24000/24000 [==============================] - 29s 1ms/step - loss: 3.7255e-04 - val_loss: 0.0023\n",
      "Epoch 166/500\n",
      "24000/24000 [==============================] - 28s 1ms/step - loss: 3.1457e-04 - val_loss: 0.0023\n",
      "Epoch 167/500\n",
      "24000/24000 [==============================] - 25s 1ms/step - loss: 2.7304e-04 - val_loss: 0.0023\n",
      "Epoch 168/500\n",
      "24000/24000 [==============================] - 29s 1ms/step - loss: 2.4452e-04 - val_loss: 0.0023\n",
      "Epoch 169/500\n",
      "24000/24000 [==============================] - 29s 1ms/step - loss: 2.2867e-04 - val_loss: 0.0023\n",
      "Epoch 170/500\n",
      "24000/24000 [==============================] - 29s 1ms/step - loss: 2.1753e-04 - val_loss: 0.0023\n",
      "Epoch 171/500\n",
      "24000/24000 [==============================] - 29s 1ms/step - loss: 2.0954e-04 - val_loss: 0.0024\n",
      "Epoch 172/500\n",
      "24000/24000 [==============================] - 30s 1ms/step - loss: 2.0330e-04 - val_loss: 0.0024\n",
      "Epoch 173/500\n",
      "24000/24000 [==============================] - 30s 1ms/step - loss: 1.9810e-04 - val_loss: 0.0024\n",
      "Epoch 174/500\n",
      "24000/24000 [==============================] - 29s 1ms/step - loss: 1.9311e-04 - val_loss: 0.0024\n",
      "Epoch 175/500\n",
      "24000/24000 [==============================] - 30s 1ms/step - loss: 1.8894e-04 - val_loss: 0.0024\n",
      "Epoch 176/500\n",
      "24000/24000 [==============================] - 29s 1ms/step - loss: 1.8492e-04 - val_loss: 0.0024\n",
      "Epoch 177/500\n",
      "24000/24000 [==============================] - 29s 1ms/step - loss: 1.8197e-04 - val_loss: 0.0024\n",
      "Epoch 178/500\n",
      "24000/24000 [==============================] - 30s 1ms/step - loss: 1.8067e-04 - val_loss: 0.0024\n",
      "Epoch 179/500\n",
      "24000/24000 [==============================] - 30s 1ms/step - loss: 1.9660e-04 - val_loss: 0.0024\n",
      "Epoch 180/500\n",
      "24000/24000 [==============================] - 29s 1ms/step - loss: 3.0206e-04 - val_loss: 0.0024\n",
      "Epoch 181/500\n",
      "24000/24000 [==============================] - 29s 1ms/step - loss: 3.4947e-04 - val_loss: 0.0024\n",
      "Epoch 182/500\n",
      "24000/24000 [==============================] - 29s 1ms/step - loss: 2.8864e-04 - val_loss: 0.0024\n",
      "Epoch 183/500\n",
      "24000/24000 [==============================] - 30s 1ms/step - loss: 2.3430e-04 - val_loss: 0.0024\n",
      "Epoch 184/500\n",
      "24000/24000 [==============================] - 31s 1ms/step - loss: 1.9773e-04 - val_loss: 0.0024\n",
      "Epoch 185/500\n",
      "24000/24000 [==============================] - 29s 1ms/step - loss: 1.7606e-04 - val_loss: 0.0024\n",
      "Epoch 186/500\n",
      "24000/24000 [==============================] - 29s 1ms/step - loss: 1.6195e-04 - val_loss: 0.0025\n",
      "Epoch 187/500\n",
      "24000/24000 [==============================] - 30s 1ms/step - loss: 1.5174e-04 - val_loss: 0.0025\n",
      "Epoch 188/500\n",
      "24000/24000 [==============================] - 29s 1ms/step - loss: 1.4548e-04 - val_loss: 0.0025\n",
      "Epoch 189/500\n",
      "24000/24000 [==============================] - 29s 1ms/step - loss: 1.3918e-04 - val_loss: 0.0025\n",
      "Epoch 190/500\n",
      "24000/24000 [==============================] - 28s 1ms/step - loss: 1.3402e-04 - val_loss: 0.0025\n",
      "Epoch 191/500\n",
      "24000/24000 [==============================] - 30s 1ms/step - loss: 1.2997e-04 - val_loss: 0.0025\n",
      "Epoch 192/500\n",
      "24000/24000 [==============================] - 30s 1ms/step - loss: 1.2546e-04 - val_loss: 0.0025\n",
      "Epoch 193/500\n",
      "24000/24000 [==============================] - 29s 1ms/step - loss: 1.2250e-04 - val_loss: 0.0025\n",
      "Epoch 194/500\n",
      "24000/24000 [==============================] - 33s 1ms/step - loss: 1.1929e-04 - val_loss: 0.0026\n",
      "Epoch 195/500\n",
      "24000/24000 [==============================] - 27s 1ms/step - loss: 1.1564e-04 - val_loss: 0.0026\n",
      "Epoch 196/500\n",
      "24000/24000 [==============================] - 24s 1ms/step - loss: 1.1266e-04 - val_loss: 0.0026\n",
      "Epoch 197/500\n",
      "24000/24000 [==============================] - 22s 910us/step - loss: 1.1835e-04 - val_loss: 0.0026\n",
      "Epoch 198/500\n",
      "24000/24000 [==============================] - 30s 1ms/step - loss: 2.1207e-04 - val_loss: 0.0026\n",
      "Epoch 199/500\n",
      "24000/24000 [==============================] - 21s 891us/step - loss: 3.5936e-04 - val_loss: 0.0025\n",
      "Epoch 200/500\n",
      "24000/24000 [==============================] - 21s 891us/step - loss: 2.8449e-04 - val_loss: 0.0025\n",
      "Epoch 201/500\n",
      "24000/24000 [==============================] - 22s 896us/step - loss: 2.0602e-04 - val_loss: 0.0025\n",
      "Epoch 202/500\n",
      "24000/24000 [==============================] - 21s 893us/step - loss: 1.5981e-04 - val_loss: 0.0025\n",
      "Epoch 203/500\n",
      "24000/24000 [==============================] - 31s 1ms/step - loss: 1.2769e-04 - val_loss: 0.0026\n",
      "Epoch 204/500\n",
      "24000/24000 [==============================] - 21s 892us/step - loss: 1.0915e-04 - val_loss: 0.0026\n",
      "Epoch 205/500\n",
      "24000/24000 [==============================] - 22s 897us/step - loss: 9.8356e-05 - val_loss: 0.0026\n",
      "Epoch 206/500\n",
      "24000/24000 [==============================] - 22s 900us/step - loss: 9.0773e-05 - val_loss: 0.0026\n",
      "Epoch 207/500\n",
      "24000/24000 [==============================] - 29s 1ms/step - loss: 8.5754e-05 - val_loss: 0.0026\n",
      "Epoch 208/500\n",
      "24000/24000 [==============================] - 21s 893us/step - loss: 8.1117e-05 - val_loss: 0.0026\n",
      "Epoch 209/500\n",
      "24000/24000 [==============================] - 22s 896us/step - loss: 7.7575e-05 - val_loss: 0.0026\n",
      "Epoch 210/500\n",
      "24000/24000 [==============================] - 22s 901us/step - loss: 7.4643e-05 - val_loss: 0.0026\n",
      "Epoch 211/500\n",
      "24000/24000 [==============================] - 33s 1ms/step - loss: 7.1779e-05 - val_loss: 0.0026\n",
      "Epoch 212/500\n",
      "24000/24000 [==============================] - 21s 893us/step - loss: 6.8724e-05 - val_loss: 0.0027\n",
      "Epoch 213/500\n",
      "24000/24000 [==============================] - 21s 894us/step - loss: 6.6051e-05 - val_loss: 0.0027\n",
      "Epoch 214/500\n",
      "24000/24000 [==============================] - 22s 899us/step - loss: 6.3634e-05 - val_loss: 0.0027\n",
      "Epoch 215/500\n",
      "24000/24000 [==============================] - 27s 1ms/step - loss: 6.1540e-05 - val_loss: 0.0027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 216/500\n",
      "24000/24000 [==============================] - 21s 866us/step - loss: 5.8957e-05 - val_loss: 0.0027\n",
      "Epoch 217/500\n",
      "24000/24000 [==============================] - 24s 997us/step - loss: 5.7064e-05 - val_loss: 0.0027\n",
      "Epoch 218/500\n",
      "24000/24000 [==============================] - 36s 2ms/step - loss: 5.5316e-05 - val_loss: 0.0027\n",
      "Epoch 219/500\n",
      "24000/24000 [==============================] - 21s 860us/step - loss: 5.6122e-05 - val_loss: 0.0027\n",
      "Epoch 220/500\n",
      "24000/24000 [==============================] - 32s 1ms/step - loss: 6.9881e-05 - val_loss: 0.0027\n",
      "Epoch 221/500\n",
      "24000/24000 [==============================] - 29s 1ms/step - loss: 3.1744e-04 - val_loss: 0.0027\n",
      "Epoch 222/500\n",
      "24000/24000 [==============================] - 21s 860us/step - loss: 3.5165e-04 - val_loss: 0.0026\n",
      "Epoch 223/500\n",
      "24000/24000 [==============================] - 21s 858us/step - loss: 2.2957e-04 - val_loss: 0.00260\n",
      "Epoch 224/500\n",
      "24000/24000 [==============================] - 21s 886us/step - loss: 1.5703e-04 - val_loss: 0.0026\n",
      "Epoch 225/500\n",
      "24000/24000 [==============================] - 29s 1ms/step - loss: 1.1049e-04 - val_loss: 0.0026\n",
      "Epoch 226/500\n",
      "24000/24000 [==============================] - 21s 866us/step - loss: 8.2397e-05 - val_loss: 0.0027\n",
      "Epoch 227/500\n",
      "24000/24000 [==============================] - 21s 861us/step - loss: 6.5281e-05 - val_loss: 0.0027\n",
      "Epoch 228/500\n",
      "24000/24000 [==============================] - 22s 933us/step - loss: 5.4654e-05 - val_loss: 0.0027\n",
      "Epoch 229/500\n",
      "24000/24000 [==============================] - 21s 861us/step - loss: 4.8696e-05 - val_loss: 0.0027\n",
      "Epoch 230/500\n",
      "24000/24000 [==============================] - 23s 961us/step - loss: 4.4424e-05 - val_loss: 0.0027\n",
      "Epoch 231/500\n",
      "24000/24000 [==============================] - 21s 864us/step - loss: 4.1663e-05 - val_loss: 0.0027\n",
      "Epoch 232/500\n",
      "24000/24000 [==============================] - 27s 1ms/step - loss: 3.9646e-05 - val_loss: 0.0027\n",
      "Epoch 233/500\n",
      "24000/24000 [==============================] - 22s 918us/step - loss: 3.7825e-05 - val_loss: 0.0027\n",
      "Epoch 234/500\n",
      "24000/24000 [==============================] - 21s 865us/step - loss: 3.6326e-05 - val_loss: 0.0027\n",
      "Epoch 235/500\n",
      "24000/24000 [==============================] - 48s 2ms/step - loss: 3.4944e-05 - val_loss: 0.0027\n",
      "Epoch 236/500\n",
      "24000/24000 [==============================] - 124s 5ms/step - loss: 3.3985e-05 - val_loss: 0.0028\n",
      "Epoch 237/500\n",
      "24000/24000 [==============================] - 125s 5ms/step - loss: 3.2512e-05 - val_loss: 0.0028\n",
      "Epoch 238/500\n",
      "24000/24000 [==============================] - 125s 5ms/step - loss: 3.1752e-05 - val_loss: 0.0028\n",
      "Epoch 239/500\n",
      "24000/24000 [==============================] - 125s 5ms/step - loss: 3.0848e-05 - val_loss: 0.0028\n",
      "Epoch 240/500\n",
      "24000/24000 [==============================] - 124s 5ms/step - loss: 2.9982e-05 - val_loss: 0.0028\n",
      "Epoch 241/500\n",
      "24000/24000 [==============================] - 124s 5ms/step - loss: 2.8888e-05 - val_loss: 0.0028\n",
      "Epoch 242/500\n",
      "24000/24000 [==============================] - 128s 5ms/step - loss: 2.8250e-05 - val_loss: 0.0028\n",
      "Epoch 243/500\n",
      "24000/24000 [==============================] - 126s 5ms/step - loss: 2.7566e-05 - val_loss: 0.0028\n",
      "Epoch 244/500\n",
      "24000/24000 [==============================] - 123s 5ms/step - loss: 2.6841e-05 - val_loss: 0.0028\n",
      "Epoch 245/500\n",
      "24000/24000 [==============================] - 124s 5ms/step - loss: 2.6014e-05 - val_loss: 0.0028\n",
      "Epoch 246/500\n",
      "24000/24000 [==============================] - 123s 5ms/step - loss: 2.5716e-05 - val_loss: 0.0028\n",
      "Epoch 247/500\n",
      "24000/24000 [==============================] - 124s 5ms/step - loss: 2.6562e-05 - val_loss: 0.0028\n",
      "Epoch 248/500\n",
      "24000/24000 [==============================] - 124s 5ms/step - loss: 3.3903e-05 - val_loss: 0.0028\n",
      "Epoch 249/500\n",
      "24000/24000 [==============================] - 123s 5ms/step - loss: 3.3014e-04 - val_loss: 0.0027\n",
      "Epoch 250/500\n",
      "24000/24000 [==============================] - 123s 5ms/step - loss: 3.8790e-04 - val_loss: 0.0027\n",
      "Epoch 251/500\n",
      "24000/24000 [==============================] - 124s 5ms/step - loss: 2.2919e-04 - val_loss: 0.0027\n",
      "Epoch 252/500\n",
      "24000/24000 [==============================] - 124s 5ms/step - loss: 1.3922e-04 - val_loss: 0.0027\n",
      "Epoch 253/500\n",
      "24000/24000 [==============================] - 124s 5ms/step - loss: 9.1010e-05 - val_loss: 0.0027\n",
      "Epoch 254/500\n",
      "24000/24000 [==============================] - 124s 5ms/step - loss: 6.3763e-05 - val_loss: 0.0027\n",
      "Epoch 255/500\n",
      "24000/24000 [==============================] - 125s 5ms/step - loss: 4.5768e-05 - val_loss: 0.0027\n",
      "Epoch 256/500\n",
      "24000/24000 [==============================] - 124s 5ms/step - loss: 3.5912e-05 - val_loss: 0.0027\n",
      "Epoch 257/500\n",
      "24000/24000 [==============================] - 125s 5ms/step - loss: 3.0712e-05 - val_loss: 0.0028\n",
      "Epoch 258/500\n",
      "24000/24000 [==============================] - 122s 5ms/step - loss: 2.7104e-05 - val_loss: 0.0028\n",
      "Epoch 259/500\n",
      "24000/24000 [==============================] - 125s 5ms/step - loss: 2.5052e-05 - val_loss: 0.0028\n",
      "Epoch 260/500\n",
      "24000/24000 [==============================] - 125s 5ms/step - loss: 2.3684e-05 - val_loss: 0.0028\n",
      "Epoch 261/500\n",
      "24000/24000 [==============================] - 122s 5ms/step - loss: 2.2754e-05 - val_loss: 0.0028\n",
      "Epoch 262/500\n",
      "24000/24000 [==============================] - 123s 5ms/step - loss: 2.1942e-05 - val_loss: 0.0028\n",
      "Epoch 263/500\n",
      "24000/24000 [==============================] - 124s 5ms/step - loss: 2.1339e-05 - val_loss: 0.0028\n",
      "Epoch 264/500\n",
      "24000/24000 [==============================] - 124s 5ms/step - loss: 2.0972e-05 - val_loss: 0.0028\n",
      "Epoch 265/500\n",
      "24000/24000 [==============================] - 124s 5ms/step - loss: 2.0476e-05 - val_loss: 0.0028\n",
      "Epoch 266/500\n",
      "24000/24000 [==============================] - 125s 5ms/step - loss: 2.0112e-05 - val_loss: 0.0028\n",
      "Epoch 267/500\n",
      "24000/24000 [==============================] - 129s 5ms/step - loss: 1.9604e-05 - val_loss: 0.0028\n",
      "Epoch 268/500\n",
      "24000/24000 [==============================] - 124s 5ms/step - loss: 1.9270e-05 - val_loss: 0.0028\n",
      "Epoch 269/500\n",
      "24000/24000 [==============================] - 124s 5ms/step - loss: 1.8995e-05 - val_loss: 0.0028\n",
      "Epoch 270/500\n",
      "24000/24000 [==============================] - 12878s 537ms/step - loss: 1.8867e-05 - val_loss: 0.0028\n",
      "Epoch 271/500\n",
      "24000/24000 [==============================] - 25s 1ms/step - loss: 1.8588e-05 - val_loss: 0.0028\n",
      "Epoch 272/500\n",
      "24000/24000 [==============================] - 28s 1ms/step - loss: 1.8237e-05 - val_loss: 0.0028\n",
      "Epoch 273/500\n",
      "24000/24000 [==============================] - 20s 849us/step - loss: 1.8083e-05 - val_loss: 0.0028\n",
      "Epoch 274/500\n",
      "24000/24000 [==============================] - 24s 990us/step - loss: 1.7900e-05 - val_loss: 0.0029\n",
      "Epoch 275/500\n",
      "24000/24000 [==============================] - 27s 1ms/step - loss: 1.7713e-05 - val_loss: 0.0029\n",
      "Epoch 276/500\n",
      "24000/24000 [==============================] - 20s 831us/step - loss: 1.7542e-05 - val_loss: 0.0029\n",
      "Epoch 277/500\n",
      "24000/24000 [==============================] - 26s 1ms/step - loss: 1.7212e-05 - val_loss: 0.0029\n",
      "Epoch 278/500\n",
      "24000/24000 [==============================] - 27s 1ms/step - loss: 1.7070e-05 - val_loss: 0.0029\n",
      "Epoch 279/500\n",
      "24000/24000 [==============================] - 20s 847us/step - loss: 1.6945e-05 - val_loss: 0.0029\n",
      "Epoch 280/500\n",
      "24000/24000 [==============================] - 24s 1ms/step - loss: 1.6911e-05 - val_loss: 0.0029\n",
      "Epoch 281/500\n",
      "24000/24000 [==============================] - 23s 957us/step - loss: 1.6926e-05 - val_loss: 0.0029\n",
      "Epoch 282/500\n",
      "24000/24000 [==============================] - 25s 1ms/step - loss: 2.0159e-05 - val_loss: 0.0029\n",
      "Epoch 283/500\n",
      "24000/24000 [==============================] - 25s 1ms/step - loss: 2.7837e-04 - val_loss: 0.0028\n",
      "Epoch 284/500\n",
      "24000/24000 [==============================] - 21s 870us/step - loss: 4.0831e-04 - val_loss: 0.0028\n",
      "Epoch 285/500\n",
      "24000/24000 [==============================] - 25s 1ms/step - loss: 2.2625e-04 - val_loss: 0.0027\n",
      "Epoch 286/500\n",
      "24000/24000 [==============================] - 21s 877us/step - loss: 1.3527e-04 - val_loss: 0.0027\n",
      "Epoch 287/500\n",
      "24000/24000 [==============================] - 23s 964us/step - loss: 8.5760e-05 - val_loss: 0.0028\n",
      "Epoch 288/500\n",
      "24000/24000 [==============================] - 22s 921us/step - loss: 5.6096e-05 - val_loss: 0.0028\n",
      "Epoch 289/500\n",
      "24000/24000 [==============================] - 23s 947us/step - loss: 3.7347e-05 - val_loss: 0.0028\n",
      "Epoch 290/500\n",
      "24000/24000 [==============================] - 24s 1ms/step - loss: 2.8008e-05 - val_loss: 0.0028\n",
      "Epoch 291/500\n",
      "24000/24000 [==============================] - 25s 1ms/step - loss: 2.2725e-05 - val_loss: 0.0028\n",
      "Epoch 292/500\n",
      "24000/24000 [==============================] - 25s 1ms/step - loss: 2.0220e-05 - val_loss: 0.0028\n",
      "Epoch 293/500\n",
      "24000/24000 [==============================] - 21s 876us/step - loss: 1.8994e-05 - val_loss: 0.0028\n",
      "Epoch 294/500\n",
      "24000/24000 [==============================] - 22s 899us/step - loss: 1.8040e-05 - val_loss: 0.0028\n",
      "Epoch 295/500\n",
      "24000/24000 [==============================] - 23s 975us/step - loss: 1.7510e-05 - val_loss: 0.0028\n",
      "Epoch 296/500\n",
      "24000/24000 [==============================] - 22s 933us/step - loss: 1.7027e-05 - val_loss: 0.0028\n",
      "Epoch 297/500\n",
      "24000/24000 [==============================] - 25s 1ms/step - loss: 1.6786e-05 - val_loss: 0.0028\n",
      "Epoch 298/500\n",
      "24000/24000 [==============================] - 23s 949us/step - loss: 1.6443e-05 - val_loss: 0.0028\n",
      "Epoch 299/500\n",
      "24000/24000 [==============================] - 27s 1ms/step - loss: 1.6269e-05 - val_loss: 0.0028\n",
      "Epoch 300/500\n",
      "24000/24000 [==============================] - 21s 875us/step - loss: 1.6038e-05 - val_loss: 0.0028\n",
      "Epoch 301/500\n",
      "24000/24000 [==============================] - 28s 1ms/step - loss: 1.5879e-05 - val_loss: 0.0028\n",
      "Epoch 302/500\n",
      "24000/24000 [==============================] - 23s 979us/step - loss: 1.5607e-05 - val_loss: 0.0029\n",
      "Epoch 303/500\n",
      "24000/24000 [==============================] - 20s 839us/step - loss: 1.5458e-05 - val_loss: 0.0029\n",
      "Epoch 304/500\n",
      "24000/24000 [==============================] - 25s 1ms/step - loss: 1.5396e-05 - val_loss: 0.0029\n",
      "Epoch 305/500\n",
      "24000/24000 [==============================] - 22s 901us/step - loss: 1.5212e-05 - val_loss: 0.0029\n",
      "Epoch 306/500\n",
      "24000/24000 [==============================] - 26s 1ms/step - loss: 1.5063e-05 - val_loss: 0.0029\n",
      "Epoch 307/500\n",
      "24000/24000 [==============================] - 22s 933us/step - loss: 1.5011e-05 - val_loss: 0.0029\n",
      "Epoch 308/500\n",
      "24000/24000 [==============================] - 26s 1ms/step - loss: 1.4878e-05 - val_loss: 0.0029e\n",
      "Epoch 309/500\n",
      "24000/24000 [==============================] - 27s 1ms/step - loss: 1.4787e-05 - val_loss: 0.0029\n",
      "Epoch 310/500\n",
      "24000/24000 [==============================] - 24s 998us/step - loss: 1.4514e-05 - val_loss: 0.0029\n",
      "Epoch 311/500\n",
      "24000/24000 [==============================] - 26s 1ms/step - loss: 1.4583e-05 - val_loss: 0.0029\n",
      "Epoch 312/500\n",
      "24000/24000 [==============================] - 19s 803us/step - loss: 1.4527e-05 - val_loss: 0.0029\n",
      "Epoch 313/500\n",
      "24000/24000 [==============================] - 27s 1ms/step - loss: 1.4360e-05 - val_loss: 0.0029\n",
      "Epoch 314/500\n",
      "24000/24000 [==============================] - 21s 872us/step - loss: 1.4319e-05 - val_loss: 0.0029\n",
      "Epoch 315/500\n",
      "24000/24000 [==============================] - 23s 968us/step - loss: 1.4237e-05 - val_loss: 0.0029\n",
      "Epoch 316/500\n",
      "24000/24000 [==============================] - 25s 1ms/step - loss: 1.4114e-05 - val_loss: 0.0029\n",
      "Epoch 317/500\n",
      "24000/24000 [==============================] - 23s 967us/step - loss: 1.4100e-05 - val_loss: 0.0029\n",
      "Epoch 318/500\n",
      "24000/24000 [==============================] - 24s 989us/step - loss: 1.4087e-05 - val_loss: 0.0029\n",
      "Epoch 319/500\n",
      "24000/24000 [==============================] - 22s 920us/step - loss: 1.4061e-05 - val_loss: 0.0029\n",
      "Epoch 320/500\n",
      "24000/24000 [==============================] - 20s 851us/step - loss: 1.4068e-05 - val_loss: 0.0029\n",
      "Epoch 321/500\n",
      "24000/24000 [==============================] - 22s 901us/step - loss: 1.3881e-05 - val_loss: 0.0029\n",
      "Epoch 322/500\n",
      "24000/24000 [==============================] - 25s 1ms/step - loss: 1.3800e-05 - val_loss: 0.0029\n",
      "Epoch 323/500\n",
      "24000/24000 [==============================] - 27s 1ms/step - loss: 1.3649e-05 - val_loss: 0.0029\n",
      "Epoch 324/500\n",
      "24000/24000 [==============================] - 20s 844us/step - loss: 1.3537e-05 - val_loss: 0.0029\n",
      "Epoch 325/500\n",
      "24000/24000 [==============================] - 24s 998us/step - loss: 1.3709e-05 - val_loss: 0.0029\n",
      "Epoch 326/500\n",
      "24000/24000 [==============================] - 19s 781us/step - loss: 1.3688e-05 - val_loss: 0.0029\n",
      "Epoch 327/500\n",
      "24000/24000 [==============================] - 26s 1ms/step - loss: 1.3596e-05 - val_loss: 0.0029\n",
      "Epoch 328/500\n",
      "24000/24000 [==============================] - 23s 942us/step - loss: 1.3708e-05 - val_loss: 0.0029\n",
      "Epoch 329/500\n",
      "24000/24000 [==============================] - 22s 899us/step - loss: 1.9800e-04 - val_loss: 0.0029\n",
      "Epoch 330/500\n",
      "24000/24000 [==============================] - 25s 1ms/step - loss: 4.7528e-04 - val_loss: 0.0028\n",
      "Epoch 331/500\n",
      "24000/24000 [==============================] - 25s 1ms/step - loss: 2.7223e-04 - val_loss: 0.0028\n",
      "Epoch 332/500\n",
      "24000/24000 [==============================] - 23s 940us/step - loss: 1.5444e-04 - val_loss: 0.0028\n",
      "Epoch 333/500\n",
      "24000/24000 [==============================] - 24s 997us/step - loss: 9.1534e-05 - val_loss: 0.0028\n",
      "Epoch 334/500\n",
      "24000/24000 [==============================] - 26s 1ms/step - loss: 5.8830e-05 - val_loss: 0.0028\n",
      "Epoch 335/500\n",
      "24000/24000 [==============================] - 26s 1ms/step - loss: 3.9099e-05 - val_loss: 0.0028\n",
      "Epoch 336/500\n",
      "24000/24000 [==============================] - 24s 994us/step - loss: 2.8039e-05 - val_loss: 0.0028\n",
      "Epoch 337/500\n",
      "24000/24000 [==============================] - 23s 969us/step - loss: 2.1584e-05 - val_loss: 0.0028\n",
      "Epoch 338/500\n",
      "24000/24000 [==============================] - 23s 953us/step - loss: 1.8303e-05 - val_loss: 0.0028\n",
      "Epoch 339/500\n",
      "24000/24000 [==============================] - 22s 899us/step - loss: 1.6592e-05 - val_loss: 0.0028\n",
      "Epoch 340/500\n",
      "24000/24000 [==============================] - 23s 950us/step - loss: 1.5561e-05 - val_loss: 0.0028\n",
      "Epoch 341/500\n",
      "24000/24000 [==============================] - 20s 851us/step - loss: 1.5099e-05 - val_loss: 0.0028\n",
      "Epoch 342/500\n",
      "24000/24000 [==============================] - 21s 876us/step - loss: 1.4653e-05 - val_loss: 0.0028\n",
      "Epoch 343/500\n",
      "24000/24000 [==============================] - 23s 948us/step - loss: 1.4435e-05 - val_loss: 0.0028\n",
      "Epoch 344/500\n",
      "24000/24000 [==============================] - 20s 852us/step - loss: 1.4217e-05 - val_loss: 0.0029\n",
      "Epoch 345/500\n",
      "24000/24000 [==============================] - 28s 1ms/step - loss: 1.4030e-05 - val_loss: 0.0029\n",
      "Epoch 346/500\n",
      "24000/24000 [==============================] - 22s 937us/step - loss: 1.3870e-05 - val_loss: 0.0029\n",
      "Epoch 347/500\n",
      "24000/24000 [==============================] - 26s 1ms/step - loss: 1.3866e-05 - val_loss: 0.0029\n",
      "Epoch 348/500\n",
      "24000/24000 [==============================] - 23s 975us/step - loss: 1.3623e-05 - val_loss: 0.0029\n",
      "Epoch 349/500\n",
      "24000/24000 [==============================] - 20s 819us/step - loss: 1.3484e-05 - val_loss: 0.0029\n",
      "Epoch 350/500\n",
      "24000/24000 [==============================] - 23s 965us/step - loss: 1.3383e-05 - val_loss: 0.0029\n",
      "Epoch 351/500\n",
      "24000/24000 [==============================] - 23s 961us/step - loss: 1.3307e-05 - val_loss: 0.0029\n",
      "Epoch 352/500\n",
      "24000/24000 [==============================] - 25s 1ms/step - loss: 1.3210e-05 - val_loss: 0.0029\n",
      "Epoch 353/500\n",
      "24000/24000 [==============================] - 23s 971us/step - loss: 1.3068e-05 - val_loss: 0.0029\n",
      "Epoch 354/500\n",
      "24000/24000 [==============================] - 25s 1ms/step - loss: 1.3055e-05 - val_loss: 0.0029\n",
      "Epoch 355/500\n",
      "24000/24000 [==============================] - 22s 911us/step - loss: 1.2909e-05 - val_loss: 0.0029\n",
      "Epoch 356/500\n",
      "24000/24000 [==============================] - 20s 814us/step - loss: 1.2820e-05 - val_loss: 0.0029\n",
      "Epoch 357/500\n",
      "24000/24000 [==============================] - 23s 974us/step - loss: 1.2753e-05 - val_loss: 0.0029\n",
      "Epoch 358/500\n",
      "24000/24000 [==============================] - 21s 876us/step - loss: 1.2796e-05 - val_loss: 0.0029\n",
      "Epoch 359/500\n",
      "24000/24000 [==============================] - 22s 910us/step - loss: 1.2698e-05 - val_loss: 0.0029\n",
      "Epoch 360/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000/24000 [==============================] - 25s 1ms/step - loss: 1.2655e-05 - val_loss: 0.0029\n",
      "Epoch 361/500\n",
      "24000/24000 [==============================] - 21s 876us/step - loss: 1.2615e-05 - val_loss: 0.0029\n",
      "Epoch 362/500\n",
      "24000/24000 [==============================] - 22s 928us/step - loss: 1.2486e-05 - val_loss: 0.0029\n",
      "Epoch 363/500\n",
      "24000/24000 [==============================] - 26s 1ms/step - loss: 1.2505e-05 - val_loss: 0.0029\n",
      "Epoch 364/500\n",
      "24000/24000 [==============================] - 25s 1ms/step - loss: 1.2436e-05 - val_loss: 0.0029\n",
      "Epoch 365/500\n",
      "24000/24000 [==============================] - 30s 1ms/step - loss: 1.2475e-05 - val_loss: 0.0029\n",
      "Epoch 366/500\n",
      "24000/24000 [==============================] - 28s 1ms/step - loss: 1.2370e-05 - val_loss: 0.0029\n",
      "Epoch 367/500\n",
      "24000/24000 [==============================] - 22s 913us/step - loss: 1.2341e-05 - val_loss: 0.0029\n",
      "Epoch 368/500\n",
      "24000/24000 [==============================] - 22s 926us/step - loss: 1.2281e-05 - val_loss: 0.0029\n",
      "Epoch 369/500\n",
      "24000/24000 [==============================] - 22s 912us/step - loss: 1.2232e-05 - val_loss: 0.0029\n",
      "Epoch 370/500\n",
      "24000/24000 [==============================] - 23s 973us/step - loss: 1.2292e-05 - val_loss: 0.0029\n",
      "Epoch 371/500\n",
      "24000/24000 [==============================] - 23s 942us/step - loss: 1.2124e-05 - val_loss: 0.0029\n",
      "Epoch 372/500\n",
      "24000/24000 [==============================] - 23s 970us/step - loss: 1.2112e-05 - val_loss: 0.0029\n",
      "Epoch 373/500\n",
      "24000/24000 [==============================] - 24s 998us/step - loss: 1.2064e-05 - val_loss: 0.0029\n",
      "Epoch 374/500\n",
      "24000/24000 [==============================] - 23s 972us/step - loss: 1.1933e-05 - val_loss: 0.0029\n",
      "Epoch 375/500\n",
      "24000/24000 [==============================] - 24s 1ms/step - loss: 1.2025e-05 - val_loss: 0.0029\n",
      "Epoch 376/500\n",
      "24000/24000 [==============================] - 21s 872us/step - loss: 1.1995e-05 - val_loss: 0.0029\n",
      "Epoch 377/500\n",
      "24000/24000 [==============================] - 24s 998us/step - loss: 4.0110e-05 - val_loss: 0.0029\n",
      "Epoch 378/500\n",
      "24000/24000 [==============================] - 24s 1ms/step - loss: 4.4252e-04 - val_loss: 0.0028\n",
      "Epoch 379/500\n",
      "24000/24000 [==============================] - 22s 913us/step - loss: 3.0179e-04 - val_loss: 0.0028\n",
      "Epoch 380/500\n",
      "24000/24000 [==============================] - 22s 924us/step - loss: 1.7198e-04 - val_loss: 0.0028\n",
      "Epoch 381/500\n",
      "24000/24000 [==============================] - 22s 925us/step - loss: 1.0207e-04 - val_loss: 0.0028\n",
      "Epoch 382/500\n",
      "24000/24000 [==============================] - 21s 882us/step - loss: 6.2131e-05 - val_loss: 0.0028\n",
      "Epoch 383/500\n",
      "24000/24000 [==============================] - 24s 1ms/step - loss: 4.0512e-05 - val_loss: 0.0028\n",
      "Epoch 384/500\n",
      "24000/24000 [==============================] - 22s 937us/step - loss: 2.8474e-05 - val_loss: 0.0028\n",
      "Epoch 385/500\n",
      "24000/24000 [==============================] - 22s 914us/step - loss: 2.1341e-05 - val_loss: 0.0028\n",
      "Epoch 386/500\n",
      "24000/24000 [==============================] - 22s 935us/step - loss: 1.7685e-05 - val_loss: 0.0028\n",
      "Epoch 387/500\n",
      "24000/24000 [==============================] - 21s 895us/step - loss: 1.5497e-05 - val_loss: 0.0028\n",
      "Epoch 388/500\n",
      "24000/24000 [==============================] - 26s 1ms/step - loss: 1.4473e-05 - val_loss: 0.0028\n",
      "Epoch 389/500\n",
      "24000/24000 [==============================] - 21s 857us/step - loss: 1.3796e-05 - val_loss: 0.0029\n",
      "Epoch 390/500\n",
      "24000/24000 [==============================] - 22s 931us/step - loss: 1.3367e-05 - val_loss: 0.0029\n",
      "Epoch 391/500\n",
      "24000/24000 [==============================] - 20s 819us/step - loss: 1.3196e-05 - val_loss: 0.0029\n",
      "Epoch 392/500\n",
      "24000/24000 [==============================] - 24s 984us/step - loss: 1.3062e-05 - val_loss: 0.0029\n",
      "Epoch 393/500\n",
      "24000/24000 [==============================] - 24s 990us/step - loss: 1.2862e-05 - val_loss: 0.0029\n",
      "Epoch 394/500\n",
      "24000/24000 [==============================] - 22s 910us/step - loss: 1.2627e-05 - val_loss: 0.0029\n",
      "Epoch 395/500\n",
      "24000/24000 [==============================] - 22s 907us/step - loss: 1.2619e-05 - val_loss: 0.0029\n",
      "Epoch 396/500\n",
      "24000/24000 [==============================] - 24s 1ms/step - loss: 1.2455e-05 - val_loss: 0.0029\n",
      "Epoch 397/500\n",
      "24000/24000 [==============================] - 25s 1ms/step - loss: 1.2324e-05 - val_loss: 0.0029\n",
      "Epoch 398/500\n",
      "24000/24000 [==============================] - 20s 851us/step - loss: 1.2309e-05 - val_loss: 0.0029\n",
      "Epoch 399/500\n",
      "24000/24000 [==============================] - 23s 948us/step - loss: 1.2186e-05 - val_loss: 0.0029\n",
      "Epoch 400/500\n",
      "24000/24000 [==============================] - 24s 1ms/step - loss: 1.2151e-05 - val_loss: 0.0029\n",
      "Epoch 401/500\n",
      "24000/24000 [==============================] - 24s 986us/step - loss: 1.2123e-05 - val_loss: 0.0029\n",
      "Epoch 402/500\n",
      "24000/24000 [==============================] - 22s 917us/step - loss: 1.1965e-05 - val_loss: 0.0029\n",
      "Epoch 403/500\n",
      "24000/24000 [==============================] - 24s 1ms/step - loss: 1.1978e-05 - val_loss: 0.0029\n",
      "Epoch 404/500\n",
      "24000/24000 [==============================] - 20s 840us/step - loss: 1.1875e-05 - val_loss: 0.0029\n",
      "Epoch 405/500\n",
      "24000/24000 [==============================] - 24s 989us/step - loss: 1.1809e-05 - val_loss: 0.0029\n",
      "Epoch 406/500\n",
      "24000/24000 [==============================] - 22s 906us/step - loss: 1.1734e-05 - val_loss: 0.0029\n",
      "Epoch 407/500\n",
      "24000/24000 [==============================] - 22s 898us/step - loss: 1.1701e-05 - val_loss: 0.0029\n",
      "Epoch 408/500\n",
      "24000/24000 [==============================] - 22s 908us/step - loss: 1.1750e-05 - val_loss: 0.0029\n",
      "Epoch 409/500\n",
      "24000/24000 [==============================] - 26s 1ms/step - loss: 1.1640e-05 - val_loss: 0.0029\n",
      "Epoch 410/500\n",
      "24000/24000 [==============================] - 27s 1ms/step - loss: 1.1529e-05 - val_loss: 0.0029\n",
      "Epoch 411/500\n",
      "24000/24000 [==============================] - 24s 1ms/step - loss: 1.1554e-05 - val_loss: 0.0029\n",
      "Epoch 412/500\n",
      "24000/24000 [==============================] - 22s 904us/step - loss: 1.1528e-05 - val_loss: 0.0029\n",
      "Epoch 413/500\n",
      "24000/24000 [==============================] - 24s 1ms/step - loss: 1.1453e-05 - val_loss: 0.0029\n",
      "Epoch 414/500\n",
      "24000/24000 [==============================] - 26s 1ms/step - loss: 1.1532e-05 - val_loss: 0.0029\n",
      "Epoch 415/500\n",
      "24000/24000 [==============================] - 21s 866us/step - loss: 1.1443e-05 - val_loss: 0.0029\n",
      "Epoch 416/500\n",
      "24000/24000 [==============================] - 22s 927us/step - loss: 1.1423e-05 - val_loss: 0.0029\n",
      "Epoch 417/500\n",
      "24000/24000 [==============================] - 21s 866us/step - loss: 1.1275e-05 - val_loss: 0.0029\n",
      "Epoch 418/500\n",
      "24000/24000 [==============================] - 25s 1ms/step - loss: 1.1301e-05 - val_loss: 0.0029\n",
      "Epoch 419/500\n",
      "24000/24000 [==============================] - 24s 983us/step - loss: 1.1211e-05 - val_loss: 0.0029\n",
      "Epoch 420/500\n",
      "24000/24000 [==============================] - 25s 1ms/step - loss: 1.1243e-05 - val_loss: 0.0029\n",
      "Epoch 421/500\n",
      "24000/24000 [==============================] - 24s 1ms/step - loss: 1.1199e-05 - val_loss: 0.0029\n",
      "Epoch 422/500\n",
      "24000/24000 [==============================] - 23s 971us/step - loss: 1.1148e-05 - val_loss: 0.0029\n",
      "Epoch 423/500\n",
      "24000/24000 [==============================] - 21s 880us/step - loss: 1.1167e-05 - val_loss: 0.0029\n",
      "Epoch 424/500\n",
      "24000/24000 [==============================] - 25s 1ms/step - loss: 1.1180e-05 - val_loss: 0.0029\n",
      "Epoch 425/500\n",
      "24000/24000 [==============================] - 19s 797us/step - loss: 1.1077e-05 - val_loss: 0.0029\n",
      "Epoch 426/500\n",
      "24000/24000 [==============================] - 23s 945us/step - loss: 1.0947e-05 - val_loss: 0.0029\n",
      "Epoch 427/500\n",
      "24000/24000 [==============================] - 23s 977us/step - loss: 1.0984e-05 - val_loss: 0.0029\n",
      "Epoch 428/500\n",
      "24000/24000 [==============================] - 21s 892us/step - loss: 1.0966e-05 - val_loss: 0.0029\n",
      "Epoch 429/500\n",
      "24000/24000 [==============================] - 22s 928us/step - loss: 1.0922e-05 - val_loss: 0.0029\n",
      "Epoch 430/500\n",
      "24000/24000 [==============================] - 27s 1ms/step - loss: 1.0905e-05 - val_loss: 0.0029\n",
      "Epoch 431/500\n",
      "24000/24000 [==============================] - 27s 1ms/step - loss: 1.0859e-05 - val_loss: 0.0029\n",
      "Epoch 432/500\n",
      "24000/24000 [==============================] - 25s 1ms/step - loss: 1.0775e-05 - val_loss: 0.0029\n",
      "Epoch 433/500\n",
      "24000/24000 [==============================] - 25s 1ms/step - loss: 1.0796e-05 - val_loss: 0.0030\n",
      "Epoch 434/500\n",
      "24000/24000 [==============================] - 21s 892us/step - loss: 1.0764e-05 - val_loss: 0.0030\n",
      "Epoch 435/500\n",
      "24000/24000 [==============================] - 23s 942us/step - loss: 1.0741e-05 - val_loss: 0.0030\n",
      "Epoch 436/500\n",
      "24000/24000 [==============================] - 33s 1ms/step - loss: 1.0713e-05 - val_loss: 0.0030\n",
      "Epoch 437/500\n",
      "24000/24000 [==============================] - 22s 931us/step - loss: 1.0723e-05 - val_loss: 0.0030\n",
      "Epoch 438/500\n",
      "24000/24000 [==============================] - 24s 1ms/step - loss: 1.0648e-05 - val_loss: 0.0030\n",
      "Epoch 439/500\n",
      "24000/24000 [==============================] - 24s 993us/step - loss: 1.0591e-05 - val_loss: 0.0030\n",
      "Epoch 440/500\n",
      "24000/24000 [==============================] - 27s 1ms/step - loss: 1.0503e-05 - val_loss: 0.0030\n",
      "Epoch 441/500\n",
      "24000/24000 [==============================] - 25s 1ms/step - loss: 1.0513e-05 - val_loss: 0.0030\n",
      "Epoch 442/500\n",
      "24000/24000 [==============================] - 24s 1ms/step - loss: 1.0570e-05 - val_loss: 0.0030\n",
      "Epoch 443/500\n",
      "24000/24000 [==============================] - 22s 901us/step - loss: 1.0461e-05 - val_loss: 0.0030\n",
      "Epoch 444/500\n",
      "24000/24000 [==============================] - 18s 762us/step - loss: 1.0709e-05 - val_loss: 0.0030\n",
      "Epoch 445/500\n",
      "24000/24000 [==============================] - 20s 851us/step - loss: 1.6645e-04 - val_loss: 0.0029\n",
      "Epoch 446/500\n",
      "24000/24000 [==============================] - 23s 971us/step - loss: 4.3805e-04 - val_loss: 0.0028\n",
      "Epoch 447/500\n",
      "24000/24000 [==============================] - 23s 975us/step - loss: 2.4365e-04 - val_loss: 0.0028\n",
      "Epoch 448/500\n",
      "24000/24000 [==============================] - 21s 896us/step - loss: 1.4050e-04 - val_loss: 0.0028\n",
      "Epoch 449/500\n",
      "24000/24000 [==============================] - 126s 5ms/step - loss: 8.4834e-05 - val_loss: 0.0028\n",
      "Epoch 450/500\n",
      "24000/24000 [==============================] - 41s 2ms/step - loss: 5.4598e-05 - val_loss: 0.0028\n",
      "Epoch 451/500\n",
      "24000/24000 [==============================] - 54s 2ms/step - loss: 3.6271e-05 - val_loss: 0.0028\n",
      "Epoch 452/500\n",
      "24000/24000 [==============================] - 50s 2ms/step - loss: 2.5004e-05 - val_loss: 0.0028\n",
      "Epoch 453/500\n",
      "24000/24000 [==============================] - 33s 1ms/step - loss: 1.8598e-05 - val_loss: 0.0029\n",
      "Epoch 454/500\n",
      "24000/24000 [==============================] - 35s 1ms/step - loss: 1.5498e-05 - val_loss: 0.0029\n",
      "Epoch 455/500\n",
      "24000/24000 [==============================] - 33s 1ms/step - loss: 1.3784e-05 - val_loss: 0.0029\n",
      "Epoch 456/500\n",
      "24000/24000 [==============================] - 34s 1ms/step - loss: 1.2893e-05 - val_loss: 0.0029\n",
      "Epoch 457/500\n",
      "24000/24000 [==============================] - 43s 2ms/step - loss: 1.2383e-05 - val_loss: 0.0029\n",
      "Epoch 458/500\n",
      "24000/24000 [==============================] - 58s 2ms/step - loss: 1.2138e-05 - val_loss: 0.0029\n",
      "Epoch 459/500\n",
      "24000/24000 [==============================] - 59s 2ms/step - loss: 1.1891e-05 - val_loss: 0.0029\n",
      "Epoch 460/500\n",
      "24000/24000 [==============================] - 60s 3ms/step - loss: 1.1812e-05 - val_loss: 0.0029\n",
      "Epoch 461/500\n",
      "24000/24000 [==============================] - 60s 3ms/step - loss: 1.1626e-05 - val_loss: 0.0029\n",
      "Epoch 462/500\n",
      "24000/24000 [==============================] - 42s 2ms/step - loss: 1.1523e-05 - val_loss: 0.0029\n",
      "Epoch 463/500\n",
      "24000/24000 [==============================] - 36s 2ms/step - loss: 1.1340e-05 - val_loss: 0.0029\n",
      "Epoch 464/500\n",
      "24000/24000 [==============================] - 36s 1ms/step - loss: 1.1304e-05 - val_loss: 0.0029\n",
      "Epoch 465/500\n",
      "24000/24000 [==============================] - 35s 1ms/step - loss: 1.1233e-05 - val_loss: 0.0029\n",
      "Epoch 466/500\n",
      "24000/24000 [==============================] - 35s 1ms/step - loss: 1.1205e-05 - val_loss: 0.0029\n",
      "Epoch 467/500\n",
      "24000/24000 [==============================] - 36s 1ms/step - loss: 1.1027e-05 - val_loss: 0.0029\n",
      "Epoch 468/500\n",
      "24000/24000 [==============================] - 35s 1ms/step - loss: 1.0993e-05 - val_loss: 0.0029\n",
      "Epoch 469/500\n",
      "24000/24000 [==============================] - 34s 1ms/step - loss: 1.0983e-05 - val_loss: 0.0029\n",
      "Epoch 470/500\n",
      "24000/24000 [==============================] - 36s 1ms/step - loss: 1.0944e-05 - val_loss: 0.0029\n",
      "Epoch 471/500\n",
      "24000/24000 [==============================] - 36s 1ms/step - loss: 1.0913e-05 - val_loss: 0.0029\n",
      "Epoch 472/500\n",
      "24000/24000 [==============================] - 37s 2ms/step - loss: 1.0883e-05 - val_loss: 0.0029\n",
      "Epoch 473/500\n",
      "24000/24000 [==============================] - 35s 1ms/step - loss: 1.0848e-05 - val_loss: 0.0029\n",
      "Epoch 474/500\n",
      "24000/24000 [==============================] - 37s 2ms/step - loss: 1.0822e-05 - val_loss: 0.0029\n",
      "Epoch 475/500\n",
      "24000/24000 [==============================] - 37s 2ms/step - loss: 1.0784e-05 - val_loss: 0.0029\n",
      "Epoch 476/500\n",
      "24000/24000 [==============================] - 37s 2ms/step - loss: 1.0719e-05 - val_loss: 0.0029\n",
      "Epoch 477/500\n",
      "24000/24000 [==============================] - 37s 2ms/step - loss: 1.0709e-05 - val_loss: 0.0029\n",
      "Epoch 478/500\n",
      "24000/24000 [==============================] - 37s 2ms/step - loss: 1.0678e-05 - val_loss: 0.0029\n",
      "Epoch 479/500\n",
      "24000/24000 [==============================] - 38s 2ms/step - loss: 1.0584e-05 - val_loss: 0.0029\n",
      "Epoch 480/500\n",
      "24000/24000 [==============================] - 37s 2ms/step - loss: 1.0548e-05 - val_loss: 0.0029\n",
      "Epoch 481/500\n",
      "24000/24000 [==============================] - 39s 2ms/step - loss: 1.0572e-05 - val_loss: 0.0029\n",
      "Epoch 482/500\n",
      "24000/24000 [==============================] - 37s 2ms/step - loss: 1.0528e-05 - val_loss: 0.0029\n",
      "Epoch 483/500\n",
      "24000/24000 [==============================] - 38s 2ms/step - loss: 1.0514e-05 - val_loss: 0.0029\n",
      "Epoch 484/500\n",
      "24000/24000 [==============================] - 37s 2ms/step - loss: 1.0468e-05 - val_loss: 0.0029\n",
      "Epoch 485/500\n",
      "24000/24000 [==============================] - 37s 2ms/step - loss: 1.0447e-05 - val_loss: 0.0029\n",
      "Epoch 486/500\n",
      "24000/24000 [==============================] - 37s 2ms/step - loss: 1.0426e-05 - val_loss: 0.0029\n",
      "Epoch 487/500\n",
      "24000/24000 [==============================] - 37s 2ms/step - loss: 1.0385e-05 - val_loss: 0.0029\n",
      "Epoch 488/500\n",
      "24000/24000 [==============================] - 36s 2ms/step - loss: 1.0381e-05 - val_loss: 0.0029\n",
      "Epoch 489/500\n",
      "24000/24000 [==============================] - 36s 2ms/step - loss: 1.0357e-05 - val_loss: 0.0029\n",
      "Epoch 490/500\n",
      "24000/24000 [==============================] - 37s 2ms/step - loss: 1.0308e-05 - val_loss: 0.0029\n",
      "Epoch 491/500\n",
      "24000/24000 [==============================] - 37s 2ms/step - loss: 1.0315e-05 - val_loss: 0.0030\n",
      "Epoch 492/500\n",
      "24000/24000 [==============================] - 37s 2ms/step - loss: 1.0233e-05 - val_loss: 0.0030\n",
      "Epoch 493/500\n",
      "24000/24000 [==============================] - 36s 2ms/step - loss: 1.0209e-05 - val_loss: 0.0030\n",
      "Epoch 494/500\n",
      "24000/24000 [==============================] - 37s 2ms/step - loss: 1.0231e-05 - val_loss: 0.0030\n",
      "Epoch 495/500\n",
      "24000/24000 [==============================] - 37s 2ms/step - loss: 1.0245e-05 - val_loss: 0.0030\n",
      "Epoch 496/500\n",
      "24000/24000 [==============================] - 36s 2ms/step - loss: 1.0137e-05 - val_loss: 0.0030\n",
      "Epoch 497/500\n",
      "24000/24000 [==============================] - 35s 1ms/step - loss: 1.0067e-05 - val_loss: 0.0030\n",
      "Epoch 498/500\n",
      "24000/24000 [==============================] - 36s 2ms/step - loss: 1.0114e-05 - val_loss: 0.0030\n",
      "Epoch 499/500\n",
      "24000/24000 [==============================] - 36s 1ms/step - loss: 1.0067e-05 - val_loss: 0.0030\n",
      "Epoch 500/500\n",
      "24000/24000 [==============================] - 36s 2ms/step - loss: 1.0063e-05 - val_loss: 0.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\niyati\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\network.py:896: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'strided_slice:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'strided_slice:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "model.fit(x=[training_encoder_input, training_decoder_input], y=[training_decoder_output],\n",
    "          validation_data=([validation_encoder_input, validation_decoder_input], [validation_decoder_output]),\n",
    "          validation_split=0.05,\n",
    "          batch_size=240, epochs=500)\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "model.save('model_atention1.h5')  # creates a HDF5 file 'my_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 294
    },
    "colab_type": "code",
    "id": "aI2vQBdtoFhC",
    "outputId": "7973f1cb-70a8-4c67-e14b-58f0304b036d"
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "# returns a compiled model\n",
    "# identical to the previous one\n",
    "model = load_model('model_atention1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JOl9KsBafmNN"
   },
   "outputs": [],
   "source": [
    "def prediction(raw_input):\n",
    "    clean_input = clean_text(raw_input)\n",
    "    input_tok = [nltk.word_tokenize(clean_input)]\n",
    "    input_tok = [input_tok[0][::-1]]  #reverseing input seq\n",
    "    encoder_input = transform(encoding, input_tok, 20)\n",
    "    decoder_input = np.zeros(shape=(len(encoder_input), OUTPUT_LENGTH))\n",
    "    decoder_input[:,0] = WORD_CODE_START\n",
    "    for i in range(1, OUTPUT_LENGTH):\n",
    "        output = model.predict([encoder_input, decoder_input]).argmax(axis=2)\n",
    "        decoder_input[:,i] = output[:,i]\n",
    "    return output\n",
    "\n",
    "def decode(decoding, vector):\n",
    "    \"\"\"\n",
    "    :param decoding: decoding dict built by word encoding\n",
    "    :param vector: an encoded vector\n",
    "    \"\"\"\n",
    "    text = ''\n",
    "    for i in vector:\n",
    "        if i == 0:\n",
    "            break\n",
    "        text += ' '\n",
    "        text += decoding[i]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 710
    },
    "colab_type": "code",
    "id": "hm7xNwD6g0aD",
    "outputId": "085a93f3-1611-426a-84bc-514579367111"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bot: a faerie's love makes anything possible.\n",
      "human:  what have you got ?\n",
      "bot: i could not sleep... i should get going i feel really weird...\n",
      "human:  what about ?\n",
      "bot: my car's in the garage.\n",
      "human:  what i mean is , do not make any of <UNK> for <UNK> <UNK> .\n",
      "bot: where are we, mean man?\n",
      "human:  who <UNK> it ?\n",
      "bot: y'know my lady lucindy?\n",
      "human:  who am the papers ?\n",
      "bot: but you are old!\n",
      "human:  who does he\n",
      "bot: you must be edward bloom.\n",
      "human:  <UNK> .\n",
      "bot: thanks for playing along. i just have to sit for a while.\n",
      "human:  what are you saying ?\n",
      "bot: a naked woman, chained in ya house?\n",
      "human:  what ...\n",
      "bot: people show their happiness in a lot of different ways.\n",
      "human:  what have you think ?\n",
      "bot: okay, how do you want to do it?\n",
      "human:  what about it ?\n",
      "bot: uhuh. i ai not riding in that trunk no minutes. why do not i just ride with you?\n",
      "human:  who needs to be playing ?\n",
      "bot: do you dream?\n",
      "human:  what happened ?\n",
      "bot: and how long have you been sleeping with mrs. windham?\n",
      "human:  what happened ?\n",
      "bot: i have been watching you work. you are the best in the place. but you know that.\n",
      "human:  what would you do ?\n",
      "bot: are you telling me how to drive?\n",
      "human:  what happened ?\n",
      "bot: i think he is following us.\n",
      "human:  what i do .\n",
      "bot: yeah, great.\n",
      "human:  what have you got ?\n",
      "bot: i was told that the family wanted examination of the head.\n",
      "human:  what time ?\n",
      "bot: same rules for everyone, sir.\n",
      "human:  what man ?\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    seq_index = np.random.randint(1, len(short_questions))\n",
    "    output = prediction(short_questions[seq_index])\n",
    "    print ('bot:', short_questions[seq_index])\n",
    "    print ('human:', decode(decoding, output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "evTWfOOEi6PB",
    "outputId": "db8cad36-1465-4c6b-e5a4-cf5c054fb235"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hey\n",
      " what i mean .\n",
      "what\n",
      " what i mean .\n",
      "mean\n",
      " what i mean .\n",
      "hey\n",
      " what i mean .\n",
      "how are u\n",
      " <UNK> hours into <UNK> .\n",
      "into what\n",
      " what i mean .\n",
      "overtrain\n",
      " what i mean .\n",
      "haha\n",
      " what i mean .\n",
      "overtrained model\n",
      " who would their money ? <UNK> inside him !\n",
      "whats inside him\n",
      " what i mean .\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "  raw_input = input()\n",
    "  output = prediction(raw_input)\n",
    "  print (decode(decoding, output[0]))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "chatbot001.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
